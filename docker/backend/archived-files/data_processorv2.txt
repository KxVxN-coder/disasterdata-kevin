import pandas as pd
import requests
import sqlite3
from collections import Counter
import json
from datetime import datetime, timedelta
from dotenv import load_dotenv
import os
import nltk
from nltk.corpus import stopwords
import inflect
import redis
import logging

logging.basicConfig(
    filename="data_processor.log",
    level=logging.INFO,
    filemode="w",
    format="%(asctime)s %(message)s"
)

class DataProcessor:
    def __init__(self):
        """Fetches data once to be used across all API calls"""
        self.redis_cli = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
        self.cache_df = self.load_cache_data()
        #all timestamps should be in pd.Timestamp format 
        self.latest_timestamp = self.load_latest_timestamp()
        self.fetch_data()

    def load_latest_timestamp(self):
        latest_timestamp = self.redis_cli.get("latest_timestamp")
        if latest_timestamp:
            self.latest_timestamp = pd.to_datetime(latest_timestamp, utc=True)
            logging.info(f"Latest timestamp ({self.latest_timestamp}) loaded successfully.")
            return self.latest_timestamp
        else:
            logging.info("No latest timestamp found.")
            return None
        
    def load_cache_data(self):
        cache_data = self.redis_cli.get("cache_data")
        if cache_data:
            logging.info("Cache data loaded successfully.")
            return pd.read_json(cache_data)
        else:
            logging.info("No cache data found. Creating new DataFrame.")
            return pd.DataFrame()
        
    def fetch_data(self):
        """Fetching data from the API URL and converting to dataframe"""
        params={} 
        if self.latest_timestamp:
            params["start_timestamp"] = self.latest_timestamp.strftime("%a, %d %b %Y %H:%M:%S GMT")
            logging.info(f"Fetching data from API with params: {params}")
            response = requests.get("http://disasterdata.duckdns.org:5001/get_latest_posts", params=params)

            if response.status_code != 200:
                logging.error(f"Failed to fetch data with status code {response.status_code}")
                return None
        else:
            response = requests.get("http://disasterdata.duckdns.org:5001/get_latest_posts")
            logging.info(f"Latest timestamp updated to: {self.latest_timestamp}")
        
        logging.info("Data fetched from API successfully with status code 200.")
        data = response.json()
        new_latest_timestamp = pd.to_datetime(data.get("latest_timestamp"), utc=True)
        if self.latest_timestamp is None or new_latest_timestamp > self.latest_timestamp:
            self.latest_timestamp = pd.to_datetime(data["latest_timestamp"], utc = True)
            logging.info(f"Latest timestamp updated to: {self.latest_timestamp}")
            
        posts = data.get("posts", [])
        new_df = pd.DataFrame(posts)

        new_df["timestamp"] = pd.to_datetime(new_df["timestamp"], utc=True)
        #self.latest_timestamp = new_df["timestamp"].max()
        #logging.info(f"Latest timestamp updated to: {self.latest_timestamp}")

        #Merge with existing df
        self.cache_df = pd.concat([self.cache_df, new_df], ignore_index=True)
        self.redis_cli.set("cache_data", self.cache_df.to_json(orient="records"))
        logging.info("New data fetched and cached into Redis.")
        self.redis_cli.set("latest_timestamp", self.latest_timestamp.strftime("%a, %d %b %Y %H:%M:%S GMT"))
        logging.info("New latest timestamp cached into Redis.") 

        return self.cache_df
if __name__ == "__main__":
    processor = DataProcessor()
    processor.redis_cli.flushall()
    df = processor.fetch_data()
    logging.info(f"{df.size} records fetched from API.")  
    logging.info(f"Latest timestamp: {processor.latest_timestamp}") 
    cached_data = processor.cache_df
    logging.info(f"Cached data: {cached_data.head(5)}")  

